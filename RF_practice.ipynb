{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab013d6",
   "metadata": {},
   "source": [
    "# Resources for Understanding the concept: \n",
    "    1. RFL Concept: https://spinningup.openai.com/en/latest/\n",
    "    2. Tutorial Coding: https://www.youtube.com/watch?v=Mut_u40Sqz4&t=1903s\n",
    "    3. Lecture on RFL: https://www.youtube.com/watch?v=to-lHJfK4pw&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=5\n",
    "    4. About Stable-Baseline: https://stable-baselines3.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0230f",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ab66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f2b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547eea7c",
   "metadata": {},
   "source": [
    "# 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e57036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v0\" \n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77ddcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Discrete(2)\n",
      "[4.8               inf 0.41887903        inf]\n",
      "[-4.8               -inf -0.41887903        -inf]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d278f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:491: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "/home/irfan/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: 19.0\n",
      "Episode: 1 Score: 23.0\n",
      "Episode: 2 Score: 16.0\n",
      "Episode: 3 Score: 36.0\n",
      "Episode: 4 Score: 15.0\n",
      "Episode: 5 Score: 14.0\n"
     ]
    }
   ],
   "source": [
    "episode =5\n",
    "for episode in range(episode+1):\n",
    "    # 1. Update: env.reset() now returns (state, info)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        score += reward\n",
    "    print(f\"Episode: {episode} Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cba14",
   "metadata": {},
   "source": [
    "# Understanding the Environment\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8551f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8934c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94f66b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299e314f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.2001326, -0.5111328, -0.3131615, -1.3841342], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37a20f",
   "metadata": {},
   "source": [
    "# 3. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f20456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your directory for storing the logs\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30c933d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2227db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/gymnasium/envs/registration.py:512: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e7c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "PPO(\n",
      "    policy: str | type[stable_baselines3.common.policies.ActorCriticPolicy],\n",
      "    env: gymnasium.core.Env | ForwardRef(\u001b[33m'VecEnv'\u001b[39m) | str,\n",
      "    learning_rate: float | Callable[[float], float] = \u001b[32m0.0003\u001b[39m,\n",
      "    n_steps: int = \u001b[32m2048\u001b[39m,\n",
      "    batch_size: int = \u001b[32m64\u001b[39m,\n",
      "    n_epochs: int = \u001b[32m10\u001b[39m,\n",
      "    gamma: float = \u001b[32m0.99\u001b[39m,\n",
      "    gae_lambda: float = \u001b[32m0.95\u001b[39m,\n",
      "    clip_range: float | Callable[[float], float] = \u001b[32m0.2\u001b[39m,\n",
      "    clip_range_vf: \u001b[38;5;28;01mNone\u001b[39;00m | float | Callable[[float], float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    normalize_advantage: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    ent_coef: float = \u001b[32m0.0\u001b[39m,\n",
      "    vf_coef: float = \u001b[32m0.5\u001b[39m,\n",
      "    max_grad_norm: float = \u001b[32m0.5\u001b[39m,\n",
      "    use_sde: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    sde_sample_freq: int = -\u001b[32m1\u001b[39m,\n",
      "    rollout_buffer_class: type[stable_baselines3.common.buffers.RolloutBuffer] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rollout_buffer_kwargs: dict[str, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    target_kl: float | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    stats_window_size: int = \u001b[32m100\u001b[39m,\n",
      "    tensorboard_log: str | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    policy_kwargs: dict[str, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose: int = \u001b[32m0\u001b[39m,\n",
      "    seed: int | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    device: torch.device | str = \u001b[33m'auto'\u001b[39m,\n",
      "    _init_setup_model: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m PPO(OnPolicyAlgorithm):\n",
      "    \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m    Proximal Policy Optimization algorithm (PPO) (clip version)\u001b[39m\n",
      "\n",
      "\u001b[33m    Paper: https://arxiv.org/abs/1707.06347\u001b[39m\n",
      "\u001b[33m    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\u001b[39m\n",
      "\u001b[33m    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\u001b[39m\n",
      "\u001b[33m    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\u001b[39m\n",
      "\n",
      "\u001b[33m    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\u001b[39m\n",
      "\n",
      "\u001b[33m    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\u001b[39m\n",
      "\u001b[33m    :param env: The environment to learn from (if registered in Gym, can be str)\u001b[39m\n",
      "\u001b[33m    :param learning_rate: The learning rate, it can be a function\u001b[39m\n",
      "\u001b[33m        of the current progress remaining (from 1 to 0)\u001b[39m\n",
      "\u001b[33m    :param n_steps: The number of steps to run for each environment per update\u001b[39m\n",
      "\u001b[33m        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\u001b[39m\n",
      "\u001b[33m        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\u001b[39m\n",
      "\u001b[33m        See https://github.com/pytorch/pytorch/issues/29372\u001b[39m\n",
      "\u001b[33m    :param batch_size: Minibatch size\u001b[39m\n",
      "\u001b[33m    :param n_epochs: Number of epoch when optimizing the surrogate loss\u001b[39m\n",
      "\u001b[33m    :param gamma: Discount factor\u001b[39m\n",
      "\u001b[33m    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\u001b[39m\n",
      "\u001b[33m    :param clip_range: Clipping parameter, it can be a function of the current progress\u001b[39m\n",
      "\u001b[33m        remaining (from 1 to 0).\u001b[39m\n",
      "\u001b[33m    :param clip_range_vf: Clipping parameter for the value function,\u001b[39m\n",
      "\u001b[33m        it can be a function of the current progress remaining (from 1 to 0).\u001b[39m\n",
      "\u001b[33m        This is a parameter specific to the OpenAI implementation. If None is passed (default),\u001b[39m\n",
      "\u001b[33m        no clipping will be done on the value function.\u001b[39m\n",
      "\u001b[33m        IMPORTANT: this clipping depends on the reward scaling.\u001b[39m\n",
      "\u001b[33m    :param normalize_advantage: Whether to normalize or not the advantage\u001b[39m\n",
      "\u001b[33m    :param ent_coef: Entropy coefficient for the loss calculation\u001b[39m\n",
      "\u001b[33m    :param vf_coef: Value function coefficient for the loss calculation\u001b[39m\n",
      "\u001b[33m    :param max_grad_norm: The maximum value for the gradient clipping\u001b[39m\n",
      "\u001b[33m    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\u001b[39m\n",
      "\u001b[33m        instead of action noise exploration (default: False)\u001b[39m\n",
      "\u001b[33m    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\u001b[39m\n",
      "\u001b[33m        Default: -1 (only sample at the beginning of the rollout)\u001b[39m\n",
      "\u001b[33m    :param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\u001b[39m\n",
      "\u001b[33m    :param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation\u001b[39m\n",
      "\u001b[33m    :param target_kl: Limit the KL divergence between updates,\u001b[39m\n",
      "\u001b[33m        because the clipping is not enough to prevent large update\u001b[39m\n",
      "\u001b[33m        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\u001b[39m\n",
      "\u001b[33m        By default, there is no limit on the kl div.\u001b[39m\n",
      "\u001b[33m    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\u001b[39m\n",
      "\u001b[33m        the reported success rate, mean episode length, and mean reward over\u001b[39m\n",
      "\u001b[33m    :param tensorboard_log: the log location for tensorboard (if None, no logging)\u001b[39m\n",
      "\u001b[33m    :param policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`ppo_policies`\u001b[39m\n",
      "\u001b[33m    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\u001b[39m\n",
      "\u001b[33m        debug messages\u001b[39m\n",
      "\u001b[33m    :param seed: Seed for the pseudo random generators\u001b[39m\n",
      "\u001b[33m    :param device: Device (cpu, cuda, ...) on which the code should be run.\u001b[39m\n",
      "\u001b[33m        Setting it to auto, the code will be run on the GPU if possible.\u001b[39m\n",
      "\u001b[33m    :param _init_setup_model: Whether or not to build the network at the creation of the instance\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    policy_aliases: ClassVar[dict[str, type[BasePolicy]]] = {\n",
      "        \u001b[33m\"MlpPolicy\"\u001b[39m: ActorCriticPolicy,\n",
      "        \u001b[33m\"CnnPolicy\"\u001b[39m: ActorCriticCnnPolicy,\n",
      "        \u001b[33m\"MultiInputPolicy\"\u001b[39m: MultiInputActorCriticPolicy,\n",
      "    }\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        policy: Union[str, type[ActorCriticPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = \u001b[32m3e-4\u001b[39m,\n",
      "        n_steps: int = \u001b[32m2048\u001b[39m,\n",
      "        batch_size: int = \u001b[32m64\u001b[39m,\n",
      "        n_epochs: int = \u001b[32m10\u001b[39m,\n",
      "        gamma: float = \u001b[32m0.99\u001b[39m,\n",
      "        gae_lambda: float = \u001b[32m0.95\u001b[39m,\n",
      "        clip_range: Union[float, Schedule] = \u001b[32m0.2\u001b[39m,\n",
      "        clip_range_vf: Union[\u001b[38;5;28;01mNone\u001b[39;00m, float, Schedule] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        normalize_advantage: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        ent_coef: float = \u001b[32m0.0\u001b[39m,\n",
      "        vf_coef: float = \u001b[32m0.5\u001b[39m,\n",
      "        max_grad_norm: float = \u001b[32m0.5\u001b[39m,\n",
      "        use_sde: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        sde_sample_freq: int = -\u001b[32m1\u001b[39m,\n",
      "        rollout_buffer_class: Optional[type[RolloutBuffer]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        rollout_buffer_kwargs: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        target_kl: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        stats_window_size: int = \u001b[32m100\u001b[39m,\n",
      "        tensorboard_log: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        policy_kwargs: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        verbose: int = \u001b[32m0\u001b[39m,\n",
      "        seed: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        device: Union[th.device, str] = \u001b[33m\"auto\"\u001b[39m,\n",
      "        _init_setup_model: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate=learning_rate,\n",
      "            n_steps=n_steps,\n",
      "            gamma=gamma,\n",
      "            gae_lambda=gae_lambda,\n",
      "            ent_coef=ent_coef,\n",
      "            vf_coef=vf_coef,\n",
      "            max_grad_norm=max_grad_norm,\n",
      "            use_sde=use_sde,\n",
      "            sde_sample_freq=sde_sample_freq,\n",
      "            rollout_buffer_class=rollout_buffer_class,\n",
      "            rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            _init_setup_model=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "            supported_action_spaces=(\n",
      "                spaces.Box,\n",
      "                spaces.Discrete,\n",
      "                spaces.MultiDiscrete,\n",
      "                spaces.MultiBinary,\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# because of the advantage normalization\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m normalize_advantage:\n",
      "            \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "                batch_size > \u001b[32m1\u001b[39m\n",
      "            ), \u001b[33m\"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\u001b[39m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.env \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# Check that `n_steps * n_envs > 1` to avoid NaN\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# when doing advantage normalization\u001b[39;00m\n",
      "            buffer_size = self.env.num_envs * self.n_steps\n",
      "            \u001b[38;5;28;01massert\u001b[39;00m buffer_size > \u001b[32m1\u001b[39m \u001b[38;5;28;01mor\u001b[39;00m (\n",
      "                \u001b[38;5;28;01mnot\u001b[39;00m normalize_advantage\n",
      "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
      "            \u001b[38;5;66;03m# Check that the rollout buffer size is a multiple of the mini-batch size\u001b[39;00m\n",
      "            untruncated_batches = buffer_size // batch_size\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m buffer_size % batch_size > \u001b[32m0\u001b[39m:\n",
      "                warnings.warn(\n",
      "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
      "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
      "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
      "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
      "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
      "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
      "                )\n",
      "        self.batch_size = batch_size\n",
      "        self.n_epochs = n_epochs\n",
      "        self.clip_range = clip_range\n",
      "        self.clip_range_vf = clip_range_vf\n",
      "        self.normalize_advantage = normalize_advantage\n",
      "        self.target_kl = target_kl\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _setup_model(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        super()._setup_model()\n",
      "\n",
      "        \u001b[38;5;66;03m# Initialize schedules for policy/value clipping\u001b[39;00m\n",
      "        self.clip_range = FloatSchedule(self.clip_range)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.clip_range_vf \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m isinstance(self.clip_range_vf, (float, int)):\n",
      "                \u001b[38;5;28;01massert\u001b[39;00m self.clip_range_vf > \u001b[32m0\u001b[39m, \u001b[33m\"`clip_range_vf` must be positive, \"\u001b[39m \u001b[33m\"pass `None` to deactivate vf clipping\"\u001b[39m\n",
      "\n",
      "            self.clip_range_vf = FloatSchedule(self.clip_range_vf)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m train(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m        Update policy using the currently gathered rollout buffer.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;66;03m# Switch to train mode (this affects batch norm / dropout)\u001b[39;00m\n",
      "        self.policy.set_training_mode(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "        \u001b[38;5;66;03m# Update optimizer learning rate\u001b[39;00m\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "        \u001b[38;5;66;03m# Compute current clip range\u001b[39;00m\n",
      "        clip_range = self.clip_range(self._current_progress_remaining)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# Optional: clip range for the value function\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.clip_range_vf \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "\n",
      "        entropy_losses = []\n",
      "        pg_losses, value_losses = [], []\n",
      "        clip_fractions = []\n",
      "\n",
      "        continue_training = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# train for n_epochs epochs\u001b[39;00m\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m range(self.n_epochs):\n",
      "            approx_kl_divs = []\n",
      "            \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m rollout_data \u001b[38;5;28;01min\u001b[39;00m self.rollout_buffer.get(self.batch_size):\n",
      "                actions = rollout_data.actions\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m isinstance(self.action_space, spaces.Discrete):\n",
      "                    \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n",
      "                    actions = rollout_data.actions.long().flatten()\n",
      "\n",
      "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
      "                values = values.flatten()\n",
      "                \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "                advantages = rollout_data.advantages\n",
      "                \u001b[38;5;66;03m# Normalization does not make sense if mini batchsize == 1, see GH issue #325\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.normalize_advantage \u001b[38;5;28;01mand\u001b[39;00m len(advantages) > \u001b[32m1\u001b[39m:\n",
      "                    advantages = (advantages - advantages.mean()) / (advantages.std() + \u001b[32m1e-8\u001b[39m)\n",
      "\n",
      "                \u001b[38;5;66;03m# ratio between old and new policy, should be one at the first iteration\u001b[39;00m\n",
      "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
      "\n",
      "                \u001b[38;5;66;03m# clipped surrogate loss\u001b[39;00m\n",
      "                policy_loss_1 = advantages * ratio\n",
      "                policy_loss_2 = advantages * th.clamp(ratio, \u001b[32m1\u001b[39m - clip_range, \u001b[32m1\u001b[39m + clip_range)\n",
      "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
      "\n",
      "                \u001b[38;5;66;03m# Logging\u001b[39;00m\n",
      "                pg_losses.append(policy_loss.item())\n",
      "                clip_fraction = th.mean((th.abs(ratio - \u001b[32m1\u001b[39m) > clip_range).float()).item()\n",
      "                clip_fractions.append(clip_fraction)\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.clip_range_vf \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    \u001b[38;5;66;03m# No clipping\u001b[39;00m\n",
      "                    values_pred = values\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    \u001b[38;5;66;03m# Clip the difference between old and new value\u001b[39;00m\n",
      "                    \u001b[38;5;66;03m# NOTE: this depends on the reward scaling\u001b[39;00m\n",
      "                    values_pred = rollout_data.old_values + th.clamp(\n",
      "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
      "                    )\n",
      "                \u001b[38;5;66;03m# Value loss using the TD(gae_lambda) target\u001b[39;00m\n",
      "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
      "                value_losses.append(value_loss.item())\n",
      "\n",
      "                \u001b[38;5;66;03m# Entropy loss favor exploration\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m entropy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    \u001b[38;5;66;03m# Approximate entropy when no analytical form\u001b[39;00m\n",
      "                    entropy_loss = -th.mean(-log_prob)\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    entropy_loss = -th.mean(entropy)\n",
      "\n",
      "                entropy_losses.append(entropy_loss.item())\n",
      "\n",
      "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
      "\n",
      "                \u001b[38;5;66;03m# Calculate approximate form of reverse KL Divergence for early stopping\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# and Schulman blog: http://joschu.net/blog/kl-approx.html\u001b[39;00m\n",
      "                \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n",
      "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
      "                    approx_kl_div = th.mean((th.exp(log_ratio) - \u001b[32m1\u001b[39m) - log_ratio).cpu().numpy()\n",
      "                    approx_kl_divs.append(approx_kl_div)\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.target_kl \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m approx_kl_div > \u001b[32m1.5\u001b[39m * self.target_kl:\n",
      "                    continue_training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m self.verbose >= \u001b[32m1\u001b[39m:\n",
      "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
      "                    \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "                \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n",
      "                self.policy.optimizer.zero_grad()\n",
      "                loss.backward()\n",
      "                \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n",
      "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "                self.policy.optimizer.step()\n",
      "\n",
      "            self._n_updates += \u001b[32m1\u001b[39m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m continue_training:\n",
      "                \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
      "\n",
      "        \u001b[38;5;66;03m# Logs\u001b[39;00m\n",
      "        self.logger.record(\u001b[33m\"train/entropy_loss\"\u001b[39m, np.mean(entropy_losses))\n",
      "        self.logger.record(\u001b[33m\"train/policy_gradient_loss\"\u001b[39m, np.mean(pg_losses))\n",
      "        self.logger.record(\u001b[33m\"train/value_loss\"\u001b[39m, np.mean(value_losses))\n",
      "        self.logger.record(\u001b[33m\"train/approx_kl\"\u001b[39m, np.mean(approx_kl_divs))\n",
      "        self.logger.record(\u001b[33m\"train/clip_fraction\"\u001b[39m, np.mean(clip_fractions))\n",
      "        self.logger.record(\u001b[33m\"train/loss\"\u001b[39m, loss.item())\n",
      "        self.logger.record(\u001b[33m\"train/explained_variance\"\u001b[39m, explained_var)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m hasattr(self.policy, \u001b[33m\"log_std\"\u001b[39m):\n",
      "            self.logger.record(\u001b[33m\"train/std\"\u001b[39m, th.exp(self.policy.log_std).mean().item())\n",
      "\n",
      "        self.logger.record(\u001b[33m\"train/n_updates\"\u001b[39m, self._n_updates, exclude=\u001b[33m\"tensorboard\"\u001b[39m)\n",
      "        self.logger.record(\u001b[33m\"train/clip_range\"\u001b[39m, clip_range)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.clip_range_vf \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self.logger.record(\u001b[33m\"train/clip_range_vf\"\u001b[39m, clip_range_vf)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m learn(\n",
      "        self: SelfPPO,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        log_interval: int = \u001b[32m1\u001b[39m,\n",
      "        tb_log_name: str = \u001b[33m\"PPO\"\u001b[39m,\n",
      "        reset_num_timesteps: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        progress_bar: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    ) -> SelfPPO:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\u001b[31mFile:\u001b[39m           ~/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/stable_baselines3/ppo/ppo.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "PPO??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09723200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 44   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 45   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008799583 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.0143     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.97        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010561712 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.0918      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 34.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103639085 |\n",
      "|    clip_fraction        | 0.0979       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.632       |\n",
      "|    explained_variance   | 0.245        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.2         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 50.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 230          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065729036 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.613       |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.1         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 63.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 275          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074015222 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.597       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.9         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 63.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007308526 |\n",
      "|    clip_fraction        | 0.0684      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9           |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 51.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005605879 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 28.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 44        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 411       |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0061486 |\n",
      "|    clip_fraction        | 0.0571    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.579    |\n",
      "|    explained_variance   | 0.904     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22      |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.0052   |\n",
      "|    value_loss           | 18.3      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002606689 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00204    |\n",
      "|    value_loss           | 26.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7d122b00f4d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67575316",
   "metadata": {},
   "source": [
    "# 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa6d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved_Models', 'PPO_Model_CartPole.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75ac90cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Saved_Models/PPO_Model_CartPole.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db0a74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'Training/Saved_Models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d25389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a04a9f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1b6501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 47   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 42   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7d121cb291d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7bcca9",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d329fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/miniconda3/envs/RF_Basic/lib/python3.14/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(200.0), np.float64(0.0))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cc0ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f3abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RF_Basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
